---
title: "Machine Learning for Epi: Assignment 9"
output:
  html_document: default
  word_document: default
date: "2023-03-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F,
                      fig.dim = c(12, 7))

library(lattice)
library(NHANES)
library(tidyverse)
library(caret)
library(randomForest)
library(kableExtra)
```

Using the demonstration code from today's class, repeat the analysis but use up sampling and tune hyperparameters to try to improve model performance.

You should turn in the following for the assignment

1. Turn in a final knit version of your results, clearly showing the pre and post calibration plots for all three algorithms.


2. Describe your choice of final "optimal" model including why you believe it is the best model. If you decide none of your final models are "optimal", clearly describe why.


3. Describe at least one additional evaluation you would perform if the goal was to implement this model within a clinical setting.

### Set up: Partition data into training/testing
The code chunks below loads and subsets the data, and partitions the data into a 70/30 training/testing split.

```{r}
data ("NHANES")
table(NHANES$Diabetes)

keep.var<-names(NHANES) %in% c("Age", "Race1", "Education", "Poverty", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100", "BPSysAve", "BPDiaAve", "TotChol")

NHANES.subset<-NHANES[keep.var]

str(NHANES.subset)

#Remove missings and then remove duplicates
NHANES.subset<-na.omit(NHANES.subset)
NHANES.subset<-unique(NHANES.subset)

#Check distributions
summary(NHANES.subset)
```

Our resulting dataset contains `r nrow(NHANES.subset)` observations of `r ncol(NHANES.subset)` features, with 1 containing our binary outcome variable, `Diabetes`. Based on the summary, we can see that the distribution of diabetes is quite unbalanced, with a 11.4% prevalence of cases. 

```{r partition}

set.seed(123)

training.data<-createDataPartition(NHANES.subset$Diabetes, p=0.7, list=F)
train.data<-NHANES.subset[training.data, ]
test.data<-NHANES.subset[-training.data, ]

```

We will fit 3 prediction models to generate a clinical risk score for diabetes. (feature name: `Diabetes`). 

- Model 1 (`rf.nhanes.bt`): Random Forest based on all features.

- Model 2 (`svc.nhanes`): Classification Tree based on all features.

- Model 3 (`logit.nhanes`): A logistic regression model based on all features.

The best tune for hyperparameters will be selected based on accuracy, and the three models compared for accuracy across in the training set. 

### Model fitting
#### Model 1: Random Forest with 3 values of mtry and 3 values of ntree
For the random forest, we will try 3 different values of mtry. To save computation time, we will use the best tune

```{r}
# Try mtry of all, half of all, sqrt of all, 
# Try ntree of 100, 300, 500
feat.count <- c((ncol(train.data)-1), (ncol(train.data)-1)/2, sqrt(ncol(train.data)-1))
grid.rf <- expand.grid(mtry = feat.count)

#Set 5-fold cross-validation and upsample
control.obj = trainControl(method = "cv", number = 5, sampling = "up")

tree.num <- seq(100, 500, by = 200)
results.trees <- list()
for (ntree in tree.num){
  set.seed(123)
    rf.nhanes<-train(Diabetes~., data=train.data, method="rf", trControl=control.obj, metric="Accuracy", tuneGrid=grid.rf, importance=TRUE, ntree=ntree)
    index <- toString(ntree)
  results.trees[[index]]<-rf.nhanes$results
}

output.nhanes<-bind_rows(results.trees, .id = "ntrees")
best.tune<-output.nhanes[which.max(output.nhanes[,"Accuracy"]),]
best.tune$mtry
results.trees
mtry.grid <- expand.grid(.mtry=best.tune$mtry)

set.seed(123)
rf.nhanes.bt<-train(Diabetes~., data=train.data, method="rf", trControl=control.obj, metric="Accuracy", tuneGrid=mtry.grid, importance=TRUE, ntree=as.numeric(best.tune$ntrees))
rf.nhanes.bt<-train(Diabetes~., data=train.data, method="rf", trControl=control.obj, metric="Accuracy", tuneGrid=mtry.grid, importance=TRUE, ntree=100)

confusionMatrix(rf.nhanes.bt)
varImp(rf.nhanes.bt)
varImpPlot(rf.nhanes.bt$finalModel)

rf.nhanes.bt$results %>% kbl(digits = 4) %>% 
  kable_classic("striped") 

```

#### Model 2: Support Vector Classifier

```{r}
set.seed(123)

control.obj <- trainControl(method="cv", number=5, sampling = "up", classProbs = TRUE)

#Repeat expanding the grid search
set.seed(123)

svc.nhanes<-train(Diabetes ~ ., data=train.data, method="svmLinear", trControl=control.obj, preProcess=c("center", "scale"), probability=TRUE, tuneGrid=expand.grid(C=seq(0.0001,100, length = 10)))

svc.nhanes$bestTune
svc.nhanes$results
confusionMatrix(svc.nhanes)


```

#### Model 3: Logistic Regression

```{r}
set.seed(123)

control.obj <- trainControl(method="cv", number = 5, sampling = "up")

logit.nhanes <- train(Diabetes~., data = train.data, method = "glm", family = "binomial",
                    preProcess = c("center", "scale"), trControl = control.obj)

logit.nhanes$results
confusionMatrix(logit.nhanes)
coef(logit.nhanes$finalModel)

```

### Output predicted probabilities from each of the three models applied within the testing set. 

```{r}
#Predict in test-set and output probabilities
rf.probs<-predict(rf.nhanes, test.data, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.pp<-rf.probs[,2]

svc.probs<-predict(svc.nhanes,test.data, type="prob")
svc.pp<-svc.probs[,2]

#Predict in test-set using response type
logit.probs<-predict(logit.nhanes, test.data, type="prob")
logit.pp<-logit.probs[,2]
```
### Plot and compare calibration curves across the three algorithms. 

```{r}
pred.prob<-data.frame(Class=test.data$Diabetes, logit=logit.pp, rf=rf.pp, svc=svc.pp)

calplot<-(calibration(Class ~ logit+rf+svc, data=pred.prob, class="Yes", cuts=10))

xyplot(calplot, auto.key=list(columns=3))
```

### Calibrate the probabilities from SVC and RF

Partition testing data into 2 sets: set to train calibration and then set to evaluate results

Method 1: Platt's Scaling-train a logistic regression model on the outputs of your classifier


```{r}

set.seed(123)
cal.data.index<-test.data$Diabetes%>% createDataPartition(p=0.5, list=F)
cal.data<-test.data[cal.data.index, ]
final.test.data<-test.data[-cal.data.index, ]

#Calibration of RF

#Predict on test-set without scaling to obtain raw pred prob in test set
rf.probs.nocal<-predict(rf.nhanes, final.test.data, type="prob")
rf.pp.nocal<-rf.probs.nocal[,2]

#Apply model developed on training data to calibration dataset to obtain predictions
rf.probs.cal<-predict(rf.nhanes, cal.data, type="prob")
rf.pp.cal<-rf.probs.cal[,2]

#Add to dataset with actual values from calibration data
calibrf.data.frame<-data.frame(rf.pp.cal, cal.data$Diabetes)
colnames(calibrf.data.frame)<-c("x", "y")

#Use logistic regression to model predicted probabilities from calibration data to actual vales
calibrf.model<-glm(y ~ x, data=calibrf.data.frame, family = binomial)

#Apply calibration model above to raw predicted probabilities from test set
data.test.rf<-data.frame(rf.pp.nocal)
colnames(data.test.rf)<-c("x")
platt.data.rf<-predict(calibrf.model, data.test.rf, type="response")

platt.prob.rf<-data.frame(Class=final.test.data$Diabetes, rf.platt=platt.data.rf, rf=rf.pp.nocal)

calplot.rf<-(calibration(Class ~ rf.platt+rf, data=platt.prob.rf, class="Yes", cuts=10))
xyplot(calplot.rf, auto.key=list(columns=2))

#Calibration of SVC

#Predict on test-set without scaling
svc.nocal<-predict(svc.nhanes,final.test.data, type="prob")
svc.pp.nocal<-svc.nocal[,2]


#Apply model developed on training data to calibration dataset to obtain predictions
svc.cal<-predict(svc.nhanes,cal.data, type="prob")
svc.pp.cal<-svc.cal[,2]

#Add to dataset with actual values from calibration data

calib.data.frame<-data.frame(svc.pp.cal, cal.data$Diabetes)
colnames(calib.data.frame)<-c("x", "y")
calib.model<-glm(y ~ x, data=calib.data.frame, family = binomial)

#Predict on test set using model developed in calibration
data.test<-data.frame(svc.pp.nocal)
colnames(data.test)<-c("x")
platt.data<-predict(calib.model, data.test, type="response")

platt.prob<-data.frame(Class=final.test.data$Diabetes, svc.platt=platt.data, svc=svc.pp.nocal)

calplot<-(calibration(Class ~ svc.platt+svc, data=platt.prob, class="Yes", cuts=10))
xyplot(calplot, auto.key=list(columns=2))
```

